{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/roydendsouza31/AI-Generated-Images-vs-Real-Images/blob/main/main.ipynb",
      "authorship_tag": "ABX9TyOA5pg8yWJ1AAkYPrRI3YBq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roydendsouza31/AI-Generated-Images-vs-Real-Images/blob/R1/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Generated Images vs Real Images"
      ],
      "metadata": {
        "id": "B-d3dJn7ZsoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'\n",
        "\n",
        "!kaggle datasets download -d birdy654/cifake-real-and-ai-generated-synthetic-images\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Define the path to your zip file\n",
        "file_path = '/content/cifake-real-and-ai-generated-synthetic-images.zip'  # Replace 'your_file.zip' with your file's name\n",
        "\n",
        "!mkdir CIFAKE\n",
        "\n",
        "# Unzip the file to a specific destination\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/CIFAKE')  # Replace 'destination_folder' with your desired folder"
      ],
      "metadata": {
        "id": "PhJ_x3_9XV7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/CIFAKE\" # For Kaggle notebooks. If you run locally, point this line to the CIFAKE directory\n",
        "print(\"Loading dataset from: \" + dataset_dir)"
      ],
      "metadata": {
        "id": "PmckSoLodwTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all of the data using dataset from directory\n",
        "# If there isn't enough RAM available, consider using Tensorflow Datasets\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "img_height = 32 # The dataset is all 32px but this is here just to make sure\n",
        "img_width = 32\n",
        "batch_size = 32\n",
        "\n",
        "# Load the training data\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  dataset_dir + \"/train\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "# Load the validation data\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  dataset_dir + \"/test\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "# Quick sanity check to make sure it's all loaded properly\n",
        "print(\"Training Classes:\")\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)\n",
        "\n",
        "print(\"Testing Classes:\")\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "O2mMV7OXhBqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the CNN\n",
        "layers = []\n",
        "layers.append(tf.keras.layers.Rescaling(1./255)) # Normalise pixel values\n",
        "layers.append(tf.keras.layers.Conv2D(32, 3, activation='relu'))\n",
        "layers.append(tf.keras.layers.MaxPooling2D())\n",
        "layers.append(tf.keras.layers.Flatten())\n",
        "\n",
        "# Building the ANN\n",
        "layers.append(tf.keras.layers.Dense(64, activation='relu'))\n",
        "layers.append(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Create and compile the model from layers\n",
        "model = tf.keras.Sequential(layers)\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "# Build the model so we can see a summary\n",
        "model.build(input_shape=(None, 32, 32, 3))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JXdbbBCshCVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPUs are available for training\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "8W8RWPPihO1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, train the model\n",
        "\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=5,\n",
        "  verbose=1\n",
        ")\n",
        "\n",
        "!mkdir TrainedModelSave\n",
        "model.save(\"/content/TrainedModelSave\") # Saving the model\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "rlDDTlflhRYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}